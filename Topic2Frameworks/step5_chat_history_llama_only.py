# langchain_simple_agent.py
# Program demonstrates use of LangGraph for a very simple agent.
# It writes to stdout and asks the user to enter a line of text through stdin.
# It passes the line to the LLM llama-3.2-1B-Instruct, then prints the
# what the LLM returns as text to stdout.
# The LLM should use Cuda if available, if not then if mps is available then use that,
# otherwise use cpu.
# After the LangGraph graph is created but before it executes, the program
# uses the Mermaid library to write a image of the graph to the file lg_graph.png
# The program gets the LLM llama-3.2-1B-Instruct from Hugging Face and wraps
# it for LangChain using HuggingFacePipeline.
# The code is commented in detail so a reader can understand each step.

# Import necessary libraries
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline
from langgraph.graph import StateGraph, START, END
# from typing import TypedDict
from typing import Annotated
from typing_extensions import TypedDict
from langchain.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.graph.message import add_messages

# Determine the best available device for inference
# Priority: CUDA (NVIDIA GPU) > MPS (Apple Silicon) > CPU
def get_device():
    """
    Detect and return the best available compute device.
    Returns 'cuda' for NVIDIA GPUs, 'mps' for Apple Silicon, or 'cpu' as fallback.
    """
    if torch.cuda.is_available():
        print("Using CUDA (NVIDIA GPU) for inference")
        return "cuda"
    elif torch.backends.mps.is_available():
        print("Using MPS (Apple Silicon) for inference")
        return "mps"
    else:
        print("Using CPU for inference")
        return "cpu"

# =============================================================================
# STATE DEFINITION
# =============================================================================
# The state is a TypedDict that flows through all nodes in the graph.
# Each node can read from and write to specific fields in the state.
# LangGraph automatically merges the returned dict from each node into the state.

class AgentState(TypedDict):
    """
    State object that flows through the LangGraph nodes.

    Fields:
    - user_input: The text entered by the user (set by get_user_input node)
    - should_exit: Boolean flag indicating if user wants to quit (set by get_user_input node)
    - llm_response: The response generated by the LLM (set by call_llm node)

    State Flow:
    1. Initial state: all fields empty/default
    2. After get_user_input: user_input and should_exit are populated
    3. After call_llm: llm_response is populated
    4. After print_response: state unchanged (node only reads, doesn't write)

    The graph loops continuously:
        get_user_input -> [conditional] -> call_llm -> print_response -> get_user_input
                              |
                              +-> END (if user wants to quit)
    """
    user_input: str
    should_exit: bool
    verbose: bool
    skip_input: bool
    llama_response: str
    qwen_response: str
    messages: Annotated[list[AnyMessage], add_messages]

def create_llm():
    """
    Create and configure the LLM using HuggingFace's transformers library.
    Downloads llama-3.2-1B-Instruct from HuggingFace Hub and wraps it
    for use with LangChain via HuggingFacePipeline.
    """
    # Get the optimal device for inference
    device = get_device()

    # Model identifier on HuggingFace Hub
    model_id = "meta-llama/Llama-3.2-1B-Instruct"

    print(f"Loading model: {model_id}")
    print("This may take a moment on first run as the model is downloaded...")

    # Load the tokenizer - converts text to tokens the model understands
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Load the model itself with appropriate settings for the device
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        dtype=torch.float16 if device != "cpu" else torch.float32,
        device_map=device if device == "cuda" else None,
    )

    # Move model to MPS device if using Apple Silicon
    if device == "mps":
        model = model.to(device)

    # Create a text generation pipeline that combines model and tokenizer
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,  # Maximum tokens to generate in response
        do_sample=True,      # Enable sampling for varied responses
        temperature=0.7,     # Controls randomness (lower = more deterministic)
        top_p=0.95,          # Nucleus sampling parameter
        pad_token_id=tokenizer.eos_token_id,  # Suppress pad_token_id warning
    )

    # Wrap the HuggingFace pipeline for use with LangChain
    llm = HuggingFacePipeline(pipeline=pipe)

    print("Model loaded successfully!")
    return llm, tokenizer 


def create_qwen_llm():
    """
    Create and configure a Qwen LLM using HuggingFace transformers wrapped
    for LangChain. Choose a small Qwen variant if needed for local runs.
    """
    device = get_device()

    # Example Qwen model id; adjust if unavailable locally
    model_id = "Qwen/Qwen2.5-0.5B-Instruct"

    print(f"Loading Qwen model: {model_id}")

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        dtype=torch.float16 if device != "cpu" else torch.float32,
        device_map=device if device == "cuda" else None,
    )

    if device == "mps":
        model = model.to(device)

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        top_p=0.95,
        pad_token_id=tokenizer.eos_token_id,
    )

    llm = HuggingFacePipeline(pipeline=pipe)

    print("Qwen model loaded successfully!")
    return llm

def messages_to_chat_dicts(messages: list[AnyMessage]) -> list[dict]:
    chat = []
    for m in messages:
        if isinstance(m, SystemMessage):
            chat.append({"role": "system", "content": m.content})
        elif isinstance(m, HumanMessage):
            chat.append({"role": "user", "content": m.content})
        elif isinstance(m, AIMessage):
            chat.append({"role": "assistant", "content": m.content})
        else:
            raise ValueError(f"Unknown message type: {type(m)}")
    return chat


def create_graph(llm, llm_tokenizer, qwen_llm):
    """
    Create the LangGraph state graph with three separate nodes:
    1. get_user_input: Reads input from stdin
    2. call_llm: Sends input to the LLM and gets response
    3. print_response: Prints the LLM's response to stdout

    Graph structure with conditional routing and internal loop:
        START -> get_user_input -> [conditional] -> call_llm -> print_response -+
                       ^                 |                                       |
                       |                 +-> END (if user wants to quit)         |
                       |                                                         |
                       +---------------------------------------------------------+

    The graph runs continuously until the user types 'quit', 'exit', or 'q'.
    """

    # =========================================================================
    # NODE 1: get_user_input
    # =========================================================================
    # This node reads a line of text from stdin and updates the state.
    # State changes:
    #   - user_input: Set to the text entered by the user
    #   - should_exit: Set to True if user typed quit/exit/q, False otherwise
    #   - llm_response: Unchanged (not modified by this node)
    def get_user_input(state: AgentState) -> dict:
        """
        Node that prompts the user for input via stdin.

        Reads state: Nothing (fresh input each iteration)
        Updates state:
            - user_input: The raw text entered by the user
            - should_exit: True if user wants to quit, False otherwise
        """
        # Display banner before each prompt
        print("\n" + "=" * 50)
        print("Enter your text (or 'quit' to exit):")
        print("=" * 50)

        print("\n> ", end="")
        user_input = input()

        # Check if user wants to exit
        lc = user_input.strip().lower()
        if lc in ['quit', 'exit', 'q']:
            print("Goodbye!")
            return {
                "user_input": user_input,
                "should_exit": True        # Signal to exit the graph
            }
        
        # Skip empty inputs
        if lc == '':
            if state.get("verbose", False):
                print("[TRACE] get_user_input received empty input")
            return {
                "user_input": user_input,
                "should_exit": False,
                "skip_input": True
            }

        # Handle mode toggles without calling the LLM
        if lc == 'verbose':
            print("Verbose tracing enabled.")
            return {
                "user_input": user_input,
                "should_exit": False,
                "verbose": True,
                "skip_input": True
            }

        if lc == 'quiet':
            print("Quiet mode enabled (tracing disabled).")
            return {
                "user_input": user_input,
                "should_exit": False,
                "verbose": False,
                "skip_input": True
            }

        # Default: Any other input (including empty) - continue to LLM
        if state.get("verbose", False):
            print(f"[TRACE] get_user_input -> user_input='{user_input}'")

        return {
            "user_input": user_input,
            "should_exit": False,
            "skip_input": False,
            "messages": [HumanMessage(content=user_input)]
        }

    # Note: parallel fanout removed â€” routing now sends input to only one model

    # =========================================================================
    # NODE: call_llama
    # =========================================================================
    def call_llama(state: AgentState) -> dict:
        if state.get("verbose", False):
            print(f"[TRACE] call_llama received user_input={state['user_input']!r}")

        chat = messages_to_chat_dicts(state["messages"])
        prompt = llm_tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

        if state.get("verbose", False):
            print(f"[TRACE] call_llama prompt=\n{prompt}")

        print("\nProcessing input with Llama...")
        try:
            resp = llm.invoke(prompt)
        except Exception as e:
            resp = f"<llama error: {e}>"

        return {"llama_response": resp}


    # =========================================================================
    # NODE: call_qwen
    # =========================================================================
    def call_qwen(state: AgentState) -> dict:
        user_input = state["user_input"]
        if state.get("verbose", False):
            print(f"[TRACE] call_qwen received user_input='{user_input}'")
        prompt = f"User: {user_input}\nAssistant:"
        print("\nProcessing input with Qwen...")
        try:
            resp = qwen_llm.invoke(prompt)
        except Exception as e:
            resp = f"<qwen error: {e}>"
        return {"qwen_response": resp}

    # =========================================================================
    # NODE: print_both (join)
    # =========================================================================
    # This node prints both `llama_response` and `qwen_response`. It is
    # attached as a downstream node of both model nodes so it runs after
    # both branches complete and the state merges their outputs.
    def print_both(state: AgentState) -> dict:
        if state.get("verbose", False):
            print(f"[TRACE] print_both starting (len llama={len(str(state.get('llama_response','')))}, qwen={len(str(state.get('qwen_response','')))})")
        # Print whichever model produced a response. Only one model runs per input.
        has_llama = bool(state.get("llama_response"))
        has_qwen = bool(state.get("qwen_response"))

        if not has_llama and not has_qwen:
            if state.get("verbose", False):
                print("[TRACE] print_both found no responses; skipping print")
            return {}

        print("\n" + "=" * 50)
        print("Model Output:")
        print("=" * 50)

        resp = ""

        if has_llama:
            print("\n-- Llama Response --\n")
            resp = state.get("llama_response", "<no response>")
            print(resp)

        if has_qwen:
            print("\n-- Qwen Response --\n")
            resp = state.get("qwen_response", "<no response>")
            print(resp)

        return {
            "llama_response": "", 
            "qwen_response": "",
            "messages": [AIMessage(content=str(resp))]
        }

    # (old single-LLM print node removed; use print_both as the join/printer)

    # =========================================================================
    # ROUTING FUNCTION
    # =========================================================================
    # This function examines the state and determines which node to go to next.
    # It's used for conditional edges after get_user_input.
    # Two possible routes:
    #   1. User wants to quit -> END
    #   2. User entered any input -> proceed to call_llm
    def route_after_input(state: AgentState) -> str:
        """
        Routing function that determines the next node based on state.

        Logic:
          - If `should_exit` => END
          - If `skip_input` => loop back to `get_user_input`
          - If `user_input` begins with 'hey qwen' (case-insensitive) => call_qwen
          - Otherwise => call_llama
        """
        if state.get("should_exit", False):
            nxt = END
        elif state.get("skip_input", False):
            nxt = "get_user_input"
        else:
            ui = state.get("user_input", "") or ""
            if ui.strip().lower().startswith("hey qwen"):
                # nxt = "call_qwen"
                nxt = "call_llama" # TODO: Qwen temporarily disabled for testing, undo this line later
            else:
                nxt = "call_llama"

        if state.get("verbose", False):
            print(f"[TRACE] router -> {nxt}")

        return nxt

    # =========================================================================
    # GRAPH CONSTRUCTION
    # =========================================================================
    # Create a StateGraph with our defined state structure
    graph_builder = StateGraph(AgentState)

    # Add nodes to the graph
    graph_builder.add_node("get_user_input", get_user_input)
    graph_builder.add_node("call_llama", call_llama)
    graph_builder.add_node("call_qwen", call_qwen)
    graph_builder.add_node("print_both", print_both)

    # Define edges:
    # 1. START -> get_user_input (always start by getting user input)
    graph_builder.add_edge(START, "get_user_input")

    # 2. get_user_input -> [conditional] -> call_llm OR END
    #    Uses route_after_input to decide based on state.should_exit
    graph_builder.add_conditional_edges(
        "get_user_input",
        route_after_input,
        {
            "call_llama": "call_llama",
            "call_qwen": "call_qwen",
            "get_user_input": "get_user_input",
            END: END
        }
    )

    # 4. call_llama -> print_both and call_qwen -> print_both (join)
    graph_builder.add_edge("call_llama", "print_both")
    graph_builder.add_edge("call_qwen", "print_both")

    # 5. print_both -> get_user_input (loop back for next input)
    graph_builder.add_edge("print_both", "get_user_input")

    # Compile the graph into an executable form
    graph = graph_builder.compile()

    return graph

def save_graph_image(graph, filename="lg_graph.png"):
    """
    Generate a Mermaid diagram of the graph and save it as a PNG image.
    Uses the graph's built-in Mermaid export functionality.
    """
    try:
        # Get the Mermaid PNG representation of the graph
        # This requires the 'grandalf' package for rendering
        png_data = graph.get_graph(xray=True).draw_mermaid_png()

        # Write the PNG data to file
        with open(filename, "wb") as f:
            f.write(png_data)

        print(f"Graph image saved to {filename}")
    except Exception as e:
        print(f"Could not save graph image: {e}")
        print("You may need to install additional dependencies: pip install grandalf")

def main():
    """
    Main function that orchestrates the simple agent workflow:
    1. Initialize the LLM
    2. Create the LangGraph
    3. Save the graph visualization
    4. Run the graph once (it loops internally until user quits)

    The graph handles all looping internally through its edge structure:
    - get_user_input: Prompts and reads from stdin
    - call_llm: Processes input through the LLM
    - print_response: Outputs the response, then loops back to get_user_input

    The graph only terminates when the user types 'quit', 'exit', or 'q'.
    """
    print("=" * 50)
    print("LangGraph Simple Agent with Llama-3.2-1B-Instruct")
    print("=" * 50)
    print()

    # Step 1: Create and configure the LLM
    llm, tokenizer = create_llm()
    # Also create the Qwen model
    qwen_llm = create_qwen_llm()

    # Step 2: Build the LangGraph with both LLMs
    print("\nCreating LangGraph...")
    graph = create_graph(llm, tokenizer, qwen_llm)
    print("Graph created successfully!")

    # Step 3: Save a visual representation of the graph before execution
    # This happens BEFORE any graph execution, showing the graph structure
    print("\nSaving graph visualization...")
    save_graph_image(graph)

    # Step 4: Run the graph - it will loop internally until user quits
    # Create initial state with empty/default values
    # The graph will loop continuously, updating state as it goes:
    #   - get_user_input displays banner, populates user_input and should_exit
    #   - call_llm populates llm_response
    #   - print_response displays output, then loops back to get_user_input
    initial_state: AgentState = {
        "user_input": "",
        "should_exit": False,
        "llama_response": "",
        "qwen_response": "",
        "verbose": False,
        "skip_input": False,
        "messages": [SystemMessage(content="You are a helpful assistant. Only respond to user queries as assistant. Do not answer as user.")],
    }

    # Single invocation - the graph loops internally via print_response -> get_user_input
    # The graph only exits when route_after_input returns END (user typed quit/exit/q)
    graph.invoke(initial_state)

# Entry point - only run main() if this script is executed directly
if __name__ == "__main__":
    main()